# -*- coding: utf-8 -*-
"""JustinCallahanLab2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fmab9iftqcHAji1u_ZrpatKUJT9UTiIF
"""

import numpy as np
import pandas as pd
from sklearn import linear_model

data = pd.read_csv('mtcars.csv')

"""# New Section"""

from google.colab import drive
drive.mount('/content/drive')

mpg = data.mpg.values
mpg

zs = (mpg-np.mean(mpg))/np.std(mpg)
zs

wt = data.wt.values
wt

r = 0
x = 0
while r<101:
  x = np.percentile(wt,r)
  print(r, x) 
  r+=1

x = data[['wt']]
y = data[['mpg']]
lm = linear_model.LinearRegression()
model = lm.fit(x,y)

lm.predict([[2.8]])

m = lm.coef_
b = lm.intercept_

print('The slope is:'+str(m))
print('The intercept is:'+str(n))

def compute_cost(b, m, data):
    total_cost = 0
    
    # number of datapoints in training data
    N = float(len(data))
    
    # Compute sum of squared errors
    for i in range(0, len(data)):
        x = data[i, 0]
        y = data[i, 1]
        total_cost += (y - (m * x + b)) ** 2
        
    # Return average of squared error
    return total_cost/(2*N)
def step_gradient(b_current, m_current, data, alpha):
    """takes one step down towards the minima
    
    Args:
        b_current (float): current value of b
        m_current (float): current value of m
        data (np.array): array containing the training data (x,y)
        alpha (float): learning rate / step size
    
    Returns:
        tuple: (b,m) new values of b,m
    """
    
    m_gradient = 0
    b_gradient = 0
    N = float(len(data))

    # Calculate Gradient
    for i in range(0, len(data)):
        x = data[i, 0]
        y = data[i, 1]
        m_gradient += - (2/N) * x * (y - (m_current * x + b_current))
        b_gradient += - (2/N) * (y - (m_current * x + b_current))
    
    # Update current m and b
    m_updated = m_current - alpha * m_gradient
    b_updated = b_current - alpha * b_gradient

    #Return updated parameters
    return b_updated, m_updated
def gradient_descent(data, starting_b, starting_m, learning_rate, num_iterations):
    """runs gradient descent
    
    Args:
        data (np.array): training data, containing x,y
        starting_b (float): initial value of b (random)
        starting_m (float): initial value of m (random)
        learning_rate (float): hyperparameter to adjust the step size during descent
        num_iterations (int): hyperparameter, decides the number of iterations for which gradient descent would run
    
    Returns:
        list : the first and second item are b, m respectively at which the best fit curve is obtained, the third and fourth items are two lists, which store the value of b,m as gradient descent proceeded.
    """

    # initial values
    b = starting_b
    m = starting_m
    
    # to store the cost after each iteration
    cost_graph = []
    
    # to store the value of b -> bias unit, m-> slope of line after each iteration (pred = m*x + b)
    b_progress = []
    m_progress = []
    
    # For every iteration, optimize b, m and compute its cost
    for i in range(num_iterations):
        cost_graph.append(compute_cost(b, m, data))
        b, m = step_gradient(b, m, data, learning_rate)
        b_progress.append(b)
        m_progress.append(m)
        
    return [b, m, cost_graph,b_progress,m_progress]

X = data[['wt']]
y = data[['mpg']]

learning_rate = 0.01
initial_n = 20
initial_m = -2
num_iterations = 1000
data = np.concatenate((X.values,y.values),axis=1)

n, m, cost_graph,n_progress,m_progress = gradient_descent(data, initial_n, initial_m, learning_rate, num_iterations)

#Print optimized parameters
print ('Optimized n:', n)
print ('Optimized m:', m)

#Print error with optimized parameters
print ('Minimized cost:', compute_cost(n, m, data))